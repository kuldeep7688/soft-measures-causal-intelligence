# MATCH (Measure Alignment Through Comparative Human Judgement) for FCM Extraction
---

## Description  
Recent advances in natural language processing (NLP) have enhanced the automated extraction of fuzzy cognitive maps (FCMs) from text. This project presents MATCH (Measure Alignment Through Comparative Human Judgement), a novel validation method addressing the high expressiveness and partial correctness in FCM extraction. MATCH employs the Elo rating system to rank annotations through pairwise comparisons and evaluates various thresholded similarity measures against human judgment. We then apply these measures to evaluate state-of-the-art open-source large language models (LLMs) that have been fine-tuned for FCM extraction. Our findings indicate that:

1. MATCH'ed measures successfully quantify qualitative improvements in model inferences in a semi-automated fashion.  
2. Fine-tuned LLMs are more effective at generating FCMs from text than their foundation model counterparts.

This study underscores the potential benefits of developing human-aligned evaluation measures and provides a robust methodology for validating graph-based NLP predictions.



### A Note on this Repo

## Getting Started
____

### Dependencies

### Installing

### Executing program

## Authors
____
Contributors names and contact info:
XXXX

## Version History
____
* 0.1
    * Initial Release
