slurm:
  ntasks_per_node: 4

transformers:
  cache_dir: "../../../localscratch/maryam_berijanian/cache/"
  tokenizers_parallelism: false

data:
  batch_size: 16
  max_length: 256
  num_labels: 19

model:
  type: 'roberta-base'

training:
  learning_rate: 1e-5
  num_epochs: 2
  warmup_steps: 1000
  gradient_clip_value: 1.0
  save_top_k: 3
  every_n_epochs: 1  # Saving checkpoint after every n epochs
  patience: 3  # Patience for early stopping
  log_every_n_steps: 1
  precision: "16-mixed"
  accumulate_grad_batches: 2
  subset_size: 0.1  # A subset of training data to be used, between 0 and 1
  weight_decay: 0.1

paths:
  checkpoint_dir: '../../../localscratch/maryam_berijanian/models/checkpoints/'
  best_model_dir: '../../../localscratch/maryam_berijanian/models/best/'
  data_raw_dir: '/home/berijani/NER_Maryam/data/raw/'
  processed_data_dir: '/home/berijani/NER_Maryam/data/processed/'
  log_dir: '../../../localscratch/maryam_berijanian/logs/'
